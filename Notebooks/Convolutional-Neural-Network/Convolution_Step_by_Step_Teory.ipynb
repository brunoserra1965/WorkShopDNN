{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks: Step by Step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qúe es una CNN? ¿Cómo clasifica imagenes y distingue un perro de un gato?\n",
    "\n",
    "La CNN es un tipo de Red Neuronal con aprendizaje supervisado que procesa sus capas imitando al cortex visual del ojo humano para identificar distintas características en las entradas que en definitiva hacen que pueda identificar objetos y “ver”. \n",
    "\n",
    "**¿Como lo logra?** La CNN contiene varias capas ocultas especializadas y con una jerarquía: esto quiere decir que las primeras capas pueden detectar lineas, curvas y se van especializando hasta llegar a capas más profundas que reconocen formas complejas como un rostro o la silueta de un animal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introducción\n",
    "La red neuronal deberá aprender por sí sola a reconocer una diversidad de objetos dentro de imágenes y para esto necesitaremos una gran cantidad de imágenes (más de 10.000 imágenes de gatos, otras 10.000 de perros,...) para que la red pueda captar sus características únicas y a su vez, poder generalizarlo, esto es que pueda reconocer como gato tanto a uno negro, uno blanco, un gato de frente, un gato de perfil, gato saltando, etc.\n",
    "\n",
    "<img src=\"images/cnn-01.png\" style=\"width:800px;height:500px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pixeles y neuronas\n",
    "\n",
    "Para comenzar, la red toma como entrada los pixeles de una imagen. Si tenemos una imagen con apenas 28×28 pixeles de alto y ancho, eso equivale a  784 neuronas. Y eso es si sólo tenemos 1 color (escala de grises). Si tuviéramos una imagen a color, necesitaríamos 3 canales (red, green, blue) y entonces usaríamos 28x28x3 = 2352 neuronas de entrada. Esa es nuestra capa de entrada. Para continuar con el ejemplo, supondremos que utilizamos la imagen con 1 sólo color. Y luego lo generalizaremos.\n",
    "\n",
    "Antes de alimentar la red, recordar que como entrada nos conviene normalizar los valores. Los colores de los pixeles tienen valores que van de 0 a 255, haremos una transformación de cada pixel: “valor/255” y nos quedará siempre un valor entre 0 y 1.\n",
    "\n",
    "<img src=\"images/cnn-02.png\" style=\"width:800px;height:600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convoluciones\n",
    "\n",
    "Ahora comienza el “procesado distintivo” de las CNN. Es decir, haremos las llamadas “convoluciones”: Estas consisten en tomar “grupos de pixeles cercanos” de la imagen de entrada e ir operando matemáticamente (producto escalar) contra una pequeña matriz que se llama kernel o filtro en este caso. Ese kernel supongamos que es de tamaño 3×3 pixels “recorre” todas las neuronas de entrada (de izquierda-derecha, de arriba-abajo) y genera una nueva matriz de salida, que en definitiva será nuestra nueva capa de neuronas ocultas.\n",
    "\n",
    "**Nota:** si la imagen fuera a color, el kernel realmente sería de 3x3x3 es decir: un filtro con 3 kernels de 3×3; luego  esos 3 kernels se suman (y se le suma una unidad bias) que conformarán 1 salida (cómo si fuera 1 solo canal).\n",
    "\n",
    "<img src=\"images/cnn-03.png\" style=\"width:550px;height:350px;\">\n",
    "\n",
    "El kernel tomará inicialmente valores aleatorios y se irán ajustando mediante backpropagation. Una mejora es hacer que siga una distribución normal siguiendo simetrías, pero que sus valores sean aleatorios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtro\n",
    "\n",
    "En realidad, no aplicaremos 1 sólo kernel, si no que tendremos muchos kernel (su conjunto se llama filtros). Por ejemplo una primer convolución podríamos tener 32 filtros, con lo cual realmente obtendremos 32 matrices de salida (este conjunto se conoce como “feature mapping”), cada una de 28x28x1 dando un total del 25.088 neuronas para nuestra primer capa oculta de neuronas.\n",
    "\n",
    "<img src=\"images/cnn_kernel.gif\" style=\"width:450px;height:150px;\">\n",
    "\n",
    "Aquí vemos al kernel realizando el producto matricial con la imagen de entrada y desplazando de a 1 pixel de izquierda a derecha y de arriba-abajo y va generando una nueva matriz que compone al mapa de características.\n",
    "\n",
    "A medida que vamos desplazando el kernel y vamos obteniendo una “nueva imagen” filtrada por el kernel. En esta primer convolución y siguiendo con el ejemplo anterior, es como si obtuviéramos 32 “imágenes filtradas nuevas”. Estas imágenes nuevas lo que están “dibujando” son ciertas características de la imagen original. Esto ayudará en el futuro a poder distinguir un objeto de otro (por ej. gato ó perro).\n",
    "\n",
    "<img src=\"images/CNN-04.png\" style=\"width:800px;height:320px;\">\n",
    "\n",
    "**Función RELU**\n",
    "\n",
    "<img src=\"images/relu.png\" style=\"width:300px;height:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arquitectura de una CNN\n",
    "\n",
    "<img src=\"images/Typical_cnn-1.png\" style=\"width:1000px;height:450px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "\n",
    "Vamos a implementar una capa de convolución (CONV) y pooling (POOL) usando numpy, inlcuimos forward propagation and backward propagation. \n",
    "\n",
    "**Notación**:\n",
    "- El superindice $[l]$ denota un objeto de la $l^{th}$ capa. \n",
    "    - Ejemplo: $a^{[4]}$ representa la matriz de activacion de la $4^{th}$ capa. $W^{[5]}$ y $b^{[5]}$ son los parametros de la $5^{th}$ capa.\n",
    "\n",
    "\n",
    "- El superindice $(i)$ denota un objeto del $i^{th}$ ejemplo. \n",
    "    - Ejemplo: $x^{(i)}$ es el $i^{th}$ ejemplo de entrenamiento de entrada.\n",
    "    \n",
    "    \n",
    "- El supraindice $i$ denota la $i^{th}$ entrada de un vector.\n",
    "    - Ejemplo: $a^{[l]}_i$ denota la $i^{th}$ de la capa de activación $l$, asumiendo que es una capa fully connected (FC).\n",
    "    \n",
    "    \n",
    "- $n_H$, $n_W$ y $n_C$ denota respectivamente la altura (height), el ancho (width) y el numero de canales (channels) de una capa dada. Si queremos indicar los valores de la capa $l$, debemos escribir $n_H^{[l]}$, $n_W^{[l]}$, $n_C^{[l]}$. \n",
    "- $n_{H_{prev}}$, $n_{W_{prev}}$ y $n_{C_{prev}}$ denotan respectivamente la altura (height), el ancho (width) y el numero de canales (channels) de la capa anterior. Si queremos indicar los valores de la capa $l$, esto se escribe del siguiente modo $n_H^{[l-1]}$, $n_W^{[l-1]}$, $n_C^{[l-1]}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Paquetes\n",
    "\n",
    "Primero importaremos todos los paquetes que son necesarios para la implementación de nuestra CNN. \n",
    "- [numpy](www.numpy.org).\n",
    "- [matplotlib](http://matplotlib.org) es una biblioteca para mostrar graficas en Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Esquema de trabajo\n",
    "\n",
    "¡Implementaremos los componentes básicos de una red neuronal convolucional! Cada función que implementaremos tiene instrucciones detalladas que serviran como guía de lo que hay que hacer:\n",
    "\n",
    "- Función de convolución, incluye:\n",
    "    - Zero Padding\n",
    "    - Convolve window \n",
    "    - Convolution forward\n",
    "    - Convolution backward (opcional)\n",
    "- Función de pooling, incluye:\n",
    "    - Pooling forward\n",
    "    - Create mask \n",
    "    - Distribute value\n",
    "    - Pooling backward (opcional)\n",
    "    \n",
    "En este notebook implementaremos estas funciones desde cero utilizando `numpy`. En el siguiente notebook, usaremos las funciones de TensorFlow equivalentes para crear el siguiente modelo:\n",
    "\n",
    "<img src=\"images/model.png\" style=\"width:800px;height:300px;\">\n",
    "\n",
    "**Note** Para cada función hacia adelante existe su equivalente hacia atras, por lo tanto almacenaremos algunos parametros en cache para luego utilizarlos en el backprop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Convolutional Neural Networks\n",
    "\n",
    "Una capa de convolución transforma un volumen de entrada en un volumen de salida de diferente tamaño, como se muestra a continuación.\n",
    "\n",
    "<img src=\"images/conv_nn.png\" style=\"width:350px;height:200px;\">\n",
    "\n",
    "En esta parte vamos a construir cada paso de la capa convolucional. Pero primero vamos a implementar dos funciones auxiliares: una para el zero padding y la otra para calcular la función de convolución en si. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Zero-Padding\n",
    "\n",
    "Zero-padding agrega ceros alrededor de los bordes de una imagen:\n",
    "\n",
    "<img src=\"images/PAD.png\" style=\"width:600px;height:400px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figura 1** </u><font color='purple'>  : **Zero-Padding**<br> Imagen (3 canales, RGB) con padding de valor 2. </center></caption>\n",
    "\n",
    "Los principales beneficios del padding son los siguientes:\n",
    "\n",
    "- Nos permite usar una capa CONV sin necesariamente reducir la altura y el ancho de los volúmenes. Esto es importante para construir redes más profundas, ya que de lo contrario la altura/ancho se reduciría a medida que avanzamos a las capas más profundas. Un caso especial que es importante es el padding denominado \"same\", esto genera que la altura/ancho se conserva después de una capa de CONV, se hace el padding con cierto tamaño que genera que el tamaño de la entrada sea el mismo que el de la salida. \n",
    "\n",
    "- Nos ayuda a mantener más información sobre el borde de una imagen. Sin padding, muy pocos valores en la siguiente capa se verían afectados por los píxeles sobre los bordes de una imagen.\n",
    "\n",
    "**Ejercicio**: Implementar la siguiente función, la cual realiza el padding de todas las imagenes del conjunto X con tamaño pad. [Usar np.pad](https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html). Nota: si queremos realizar el padding de la matriz \"a\" de dimensiones $(5,5,5,5,5)$ con `pad = 1` para la 2da dimensión, `pad = 3` para la 4ta dimensión y `pad = 0` para el resto, tenemos que hacer:\n",
    "```python\n",
    "a = np.pad(a, ((0,0), (1,1), (0,0), (3,3), (0,0)), mode='constant', constant_values = (0,0))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Single step of convolution \n",
    "\n",
    "En esta parte, implementaremos un solo paso de convolución, en el que aplicamos el filtro a una sola posición de la imagen. Esto se usa para construir una unidad convolucional, que hace lo siguiente: \n",
    "\n",
    "- Toma un volumen de entrada \n",
    "- Applica un filtro a cada posición de la entrada\n",
    "- Devuelve otro volumen (normalmente de diferente tamaño)\n",
    "\n",
    "<img src=\"images/Convolution_schematic.gif\" style=\"width:500px;height:300px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figura 2** </u><font color='purple'>  : **Operación de convolución**<br> con un filtro de 3x3 y un stride de 1 (stride = cantidad que moveremos la ventana en cada slide) </center></caption>\n",
    "\n",
    "En aplicaciones de computer vision, cada valor de la imagen de la izquierda corresponde a el valor de un pixel, realizar la convolución equivale a multiplicar esta ventana por los valores de los pixeles de la imagen y sumar todos estos valores más el bias. Implementaremos una funciona que realiza la convolución de una sola ventana, que se corresponde con la aplicación de un filtro sobre una posicion posible de la ventana el cual retorna un valor real para esta. \n",
    "\n",
    "Luego realizaremos este paso de convolución sobre todas las ventanas posible para completar una capa de convolucion\n",
    "\n",
    "**Ejercicio**: Implementar conv_single_step(). [Pista](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.sum.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota**: la variable b se pasara a la función como un numpy array.  Si añadimos un escalar (un float o integer) a un numpy array, el resultado sera un numpy array. En el caso particular que un numpy array contenga un unico valor, podemos castearlo a un float para convertirlo a un escalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Convolutional Neural Networks - Forward pass\n",
    "\n",
    "En el forward pass, tomaremos muchos filtros y realizaremos la convolución con el volumen de entrada. Cada 'convolución' nos dara una matriz de 2D de salida. Luego juntaremos estas salidas para obtener un volumen de 3D:\n",
    "\n",
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"images/conv_kiank.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "\n",
    "**Ejercicio**: \n",
    "Implementar la funcion que realizara la convolución entre los filtros `W` sobre la capa de activación previa `A_prev`.\n",
    "Esta función toma las siguientes entradas:\n",
    "* `A_prev`, la salida de la capa anterior (considerado como un conjunto de m); \n",
    "* Weights lo denotamos como `W`.  El filtro con tamaño de ventana `f` por `f`.\n",
    "* El vector de bias es `b`, donde cada filtro tiene su propio (solo un) bias. \n",
    "\n",
    "Además se agrega un diccionario de hyperparameters que contiene el valor del stride y del padding. \n",
    "\n",
    "**Pista**: \n",
    "1. Para seleccionar una slide de `2x2` en la esquina superior izquierda de la matriz \"a_prev\" (de dimension (5,5,3)), debemos hacer lo siguiente:\n",
    "```python\n",
    "a_slice_prev = a_prev[0:2,0:2,:]\n",
    "```\n",
    "Observar que esto genera un corte 3D que tiene altura 2, ancho 2 y profundidad 3. La profundidad es el número de canales.\n",
    "Esto es util para definir `a_slice_prev`, usar los indices `start/end` que definiremos.\n",
    "2. Para definir una slice necesitamos definir los vertices `vert_start`, `vert_end`, `horiz_start` y `horiz_end`. Esta figura puede sernos de ayuda para encontrar los vertices los cuales podemos definir usando h, w, f y s en el codigo.\n",
    "\n",
    "<img src=\"images/vert_horiz_kiank.png\" style=\"width:400px;height:300px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figura 3** </u><font color='purple'>  : **Definición de una slide usando vertical and horizontal start/end (con un filtro de 2x2)** <br> Esta figura muestra un solo canal.  </center></caption>\n",
    "\n",
    "\n",
    "**Formulas**:\n",
    "Estas formulas sobre las dimensiones del resultado de una convolución a partir de las dimensiones de la entrada:\n",
    "$$ n_H = \\lfloor \\frac{n_{H_{prev}} - f + 2 \\times pad}{stride} \\rfloor +1 $$\n",
    "$$ n_W = \\lfloor \\frac{n_{W_{prev}} - f + 2 \\times pad}{stride} \\rfloor +1 $$\n",
    "$$ n_C = \\text{number of filters used in the convolution}$$\n",
    "\n",
    "Para este ejericio, no nos preocuparemos por la implementación vectorizada, y vamos a implementar todo con for-loops ya que la idea base es lograr entender el funcionamiento de una CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consejos adicionales\n",
    "\n",
    "\n",
    "* Deberian utilizar el corte de matrices (ej.`varname[0:1,:,3:5]`) para las siguientes variables:  \n",
    "  `a_prev_pad` ,`W`, `b`  \n",
    "  Copiar el código de la función y ejecutarlo fuera de la función definida, en celdas separadas. Para comparar que el subconjunto de cada matriz es del tamaño y la dimensión que espera.\n",
    "* Para decidir cómo obtener vert_start, vert_end; horiz_start, horiz_end, recordar que estos son índices de la capa anterior.  \n",
    "  Los índices de la capa de salida se denotan con `h` y` w`.  \n",
    "* Asegúrarse de que `a_slice_prev` tenga altura, ancho y profundidad.\n",
    "* Recordar que `a_prev_pad` es un subconjunto de `A_prev_pad`.  \n",
    "  Pensar cuál debería usarse dentro de los bucles for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, la capa CONV también debe contener una activación, en cuyo caso agregaríamos la siguiente línea de código:\n",
    "\n",
    "```python\n",
    "# Aplicar la función de activación\n",
    "A[i, h, w, c] = activation(Z[i, h, w, c])\n",
    "```\n",
    "\n",
    "Esto no es necesario en este ejercicio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Pooling layer \n",
    "\n",
    "La capa de agrupación (POOL) reduce la altura y el ancho de la entrada. Ayuda a reducir el computo necesario, así como a hacer que los detectores de características sean más invariantes a la posición en la entrada. Los dos tipos de capas de agrupación son:\n",
    "\n",
    "- Max-pooling layer: en las slides de entrada aplicamos ventanas de dimensiones ($f, f$) y almacenamos el maximo valor de la ventana en la salida.\n",
    "\n",
    "- Average-pooling layer: en las slides de entrada aplicamos ventanas de dimensiones ($f, f$) y almacenamos el valor promedio de la ventana en la salida.\n",
    "\n",
    "<table>\n",
    "<td>\n",
    "<img src=\"images/max_pool1.png\" style=\"width:500px;height:300px;\">\n",
    "<td>\n",
    "\n",
    "<td>\n",
    "<img src=\"images/a_pool.png\" style=\"width:500px;height:300px;\">\n",
    "<td>\n",
    "</table>\n",
    "\n",
    "Estas capas de agrupación no tienen parámetros para que la retropropagación entrene. Sin embargo, tienen hiperparámetros como el tamaño de la ventana $f$. Esto especifica la altura y el ancho de la ventana $f \\times f$ en la que calcularía el *maximo* o *promedio*.\n",
    "\n",
    "### 4.1 - Forward Pooling\n",
    "Ahora, vamos a implementar MAX-POOL y AVG-POOL, en la misma función. \n",
    "\n",
    "**Ejercicio**: Implementar la forward pass sobre una pooling layer.\n",
    "\n",
    "**Recordatorio**:\n",
    "Las formulas sobre la dimensión del volumen de salida en relación al volumen de entrada son:\n",
    "\n",
    "$$ n_H = \\lfloor \\frac{n_{H_{prev}} - f}{stride} \\rfloor +1 $$\n",
    "\n",
    "$$ n_W = \\lfloor \\frac{n_{W_{prev}} - f}{stride} \\rfloor +1 $$\n",
    "\n",
    "$$ n_C = n_{C_{prev}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, vamos a realizar el ejericio!"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "qO8ng",
   "launcher_item_id": "7XDi8"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

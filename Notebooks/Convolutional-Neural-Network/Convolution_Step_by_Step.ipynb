{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks: Step by Step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio\n",
    "\n",
    "Vamos a implementar una capa de convolución (CONV) y pooling (POOL) usando numpy, inlcuimos forward propagation and backward propagation. \n",
    "\n",
    "**Notación**:\n",
    "- El superindice $[l]$ denota un objeto de la $l^{th}$ capa. \n",
    "    - Ejemplo: $a^{[4]}$ representa la matriz de activacion de la $4^{th}$ capa. $W^{[5]}$ y $b^{[5]}$ son los parametros de la $5^{th}$ capa.\n",
    "\n",
    "\n",
    "- El superindice $(i)$ denota un objeto del $i^{th}$ ejemplo. \n",
    "    - Ejemplo: $x^{(i)}$ es el $i^{th}$ ejemplo de entrenamiento de entrada.\n",
    "    \n",
    "    \n",
    "- El supraindice $i$ denota la $i^{th}$ entrada de un vector.\n",
    "    - Ejemplo: $a^{[l]}_i$ denota la $i^{th}$ de la capa de activación $l$, asumiendo que es una capa fully connected (FC).\n",
    "    \n",
    "    \n",
    "- $n_H$, $n_W$ y $n_C$ denota respectivamente la altura (height), el ancho (width) y el numero de canales (channels) de una capa dada. Si queremos indicar los valores de la capa $l$, debemos escribir $n_H^{[l]}$, $n_W^{[l]}$, $n_C^{[l]}$. \n",
    "- $n_{H_{prev}}$, $n_{W_{prev}}$ y $n_{C_{prev}}$ denotan respectivamente la altura (height), el ancho (width) y el numero de canales (channels) de la capa anterior. Si queremos indicar los valores de la capa $l$, esto se escribe del siguiente modo $n_H^{[l-1]}$, $n_W^{[l-1]}$, $n_C^{[l-1]}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Paquetes\n",
    "\n",
    "Primero importaremos todos los paquetes que son necesarios para la implementación de nuestra CNN. \n",
    "- [numpy](www.numpy.org).\n",
    "- [matplotlib](http://matplotlib.org) es una biblioteca para mostrar graficas en Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "from NNUrudateana import *\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Esquema de trabajo\n",
    "\n",
    "¡Implementaremos los componentes básicos de una red neuronal convolucional! Cada función que implementaremos tiene instrucciones detalladas que serviran como guía de lo que hay que hacer:\n",
    "\n",
    "- Función de convolución, incluye:\n",
    "    - Zero Padding\n",
    "    - Convolve window \n",
    "    - Convolution forward\n",
    "    - Convolution backward (opcional)\n",
    "- Función de pooling, incluye:\n",
    "    - Pooling forward\n",
    "    - Create mask \n",
    "    - Distribute value\n",
    "    - Pooling backward (opcional)\n",
    "    \n",
    "En este notebook implementaremos estas funciones desde cero utilizando `numpy`. En el siguiente notebook, usaremos las funciones de TensorFlow equivalentes para crear el siguiente modelo:\n",
    "\n",
    "<img src=\"images/model.png\" style=\"width:800px;height:300px;\">\n",
    "\n",
    "**Note** Para cada función hacia adelante existe su equivalente hacia atras, por lo tanto almacenaremos algunos parametros en cache para luego utilizarlos en el backprop. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Convolutional Neural Networks\n",
    "\n",
    "Una capa de convolución transforma un volumen de entrada en un volumen de salida de diferente tamaño, como se muestra a continuación.\n",
    "\n",
    "<img src=\"images/conv_nn.png\" style=\"width:350px;height:200px;\">\n",
    "\n",
    "En esta parte vamos a construir cada paso de la capa convolucional. Pero primero vamos a implementar dos funciones auxiliares: una para el zero padding y la otra para calcular la función de convolución en si. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Zero-Padding\n",
    "\n",
    "Zero-padding agrega ceros alrededor de los bordes de una imagen:\n",
    "\n",
    "<img src=\"images/PAD.png\" style=\"width:600px;height:400px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figura 1** </u><font color='purple'>  : **Zero-Padding**<br> Imagen (3 canales, RGB) con padding de valor 2. </center></caption>\n",
    "\n",
    "Los principales beneficios del padding son los siguientes:\n",
    "\n",
    "- Nos permite usar una capa CONV sin necesariamente reducir la altura y el ancho de los volúmenes. Esto es importante para construir redes más profundas, ya que de lo contrario la altura/ancho se reduciría a medida que avanzamos a las capas más profundas. Un caso especial que es importante es el padding denominado \"same\", esto genera que la altura/ancho se conserva después de una capa de CONV, se hace el padding con cierto tamaño que genera que el tamaño de la entrada sea el mismo que el de la salida. \n",
    "\n",
    "- Nos ayuda a mantener más información sobre el borde de una imagen. Sin padding, muy pocos valores en la siguiente capa se verían afectados por los píxeles sobre los bordes de una imagen.\n",
    "\n",
    "**Ejercicio**: Implementar la siguiente función, la cual realiza el padding de todas las imagenes del conjunto X con tamaño pad. [Usar np.pad](https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html). Nota: si queremos realizar el padding de la matriz \"a\" de dimensiones $(5,5,5,5,5)$ con `pad = 1` para la 2da dimensión, `pad = 3` para la 4ta dimensión y `pad = 0` para el resto, tenemos que hacer:\n",
    "```python\n",
    "a = np.pad(a, ((0,0), (1,1), (0,0), (3,3), (0,0)), mode='constant', constant_values = (0,0))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función: zero_pad\n",
    "\n",
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    Pad con ceros todas las imagenes del conjunto X. El padding se aplica sobre la altura y el ancho de las imagenes, \n",
    "    esto se muestra en la Figura 1.\n",
    "    \n",
    "    Parametros:\n",
    "    X -- python numpy array de dimensiones (m, n_H, n_W, n_C) representa un lote de m imagenes\n",
    "    pad -- integer, cantidad de padding alrededor de cada imagen sobre las dimensiones de los ejes vertical y horizontal \n",
    "    \n",
    "    Return:\n",
    "    X_pad -- Conjunto de imagenes con padded de dimensiones (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line)\n",
    "    X_pad = np.pad(X, ((0,0), (pad,pad), (pad,pad), (0,0)), mode='constant', constant_values = (0,0))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(4, 3, 3, 2)\n",
    "x_pad = zero_pad(x, 2)\n",
    "print (\"x.shape =\\n\", x.shape)\n",
    "print (\"x_pad.shape =\\n\", x_pad.shape)\n",
    "print (\"x[1,1] =\\n\", x[1,1])\n",
    "print (\"x_pad[1,1] =\\n\", x_pad[1,1])\n",
    "\n",
    "x_res =  np.array([[ 0.90085595, -0.68372786], [-0.12289023, -0.93576943], [-0.26788808,  0.53035547]])\n",
    "\n",
    "x_re_pad = np.array([[ 0., 0.], [ 0., 0.], [ 0., 0.], [ 0., 0.], [ 0., 0.], [ 0., 0.], [ 0., 0.]])\n",
    "\n",
    "assert(x_pad.shape == (4, 7, 7, 2))\n",
    "assert(np.isclose(x[1,1], x_res).all())\n",
    "assert(np.isclose(x_pad[1,1], x_re_pad).all())\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "axarr[0].set_title('x')\n",
    "axarr[0].imshow(x[0,:,:,0])\n",
    "axarr[1].set_title('x_pad')\n",
    "axarr[1].imshow(x_pad[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "\n",
    "```\n",
    "x.shape =\n",
    " (4, 3, 3, 2)\n",
    "x_pad.shape =\n",
    " (4, 7, 7, 2)\n",
    "x[1,1] =\n",
    " [[ 0.90085595 -0.68372786]\n",
    " [-0.12289023 -0.93576943]\n",
    " [-0.26788808  0.53035547]]\n",
    "x_pad[1,1] =\n",
    " [[ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]\n",
    " [ 0.  0.]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Single step of convolution \n",
    "\n",
    "En esta parte, implementaremos un solo paso de convolución, en el que aplicamos el filtro a una sola posición de la imagen. Esto se usa para construir una unidad convolucional, que hace lo siguiente: \n",
    "\n",
    "- Toma un volumen de entrada \n",
    "- Applica un filtro a cada posición de la entrada\n",
    "- Devuelve otro volumen (normalmente de diferente tamaño)\n",
    "\n",
    "<img src=\"images/Convolution_schematic.gif\" style=\"width:500px;height:300px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figura 2** </u><font color='purple'>  : **Operación de convolución**<br> con un filtro de 3x3 y un stride de 1 (stride = cantidad que moveremos la ventana en cada slide) </center></caption>\n",
    "\n",
    "En aplicaciones de computer vision, cada valor de la imagen de la izquierda corresponde a el valor de un pixel, realizar la convolución equivale a multiplicar esta ventana por los valores de los pixeles de la imagen y sumar todos estos valores más el bias. Implementaremos una funciona que realiza la convolución de una sola ventana, que se corresponde con la aplicación de un filtro sobre una posicion posible de la ventana el cual retorna un valor real para esta. \n",
    "\n",
    "Luego realizaremos este paso de convolución sobre todas las ventanas posible para completar una capa de convolucion\n",
    "\n",
    "**Ejercicio**: Implementar conv_single_step(). [Pista](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.sum.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nota**: la variable b se pasara a la función como un numpy array.  Si añadimos un escalar (un float o integer) a un numpy array, el resultado sera un numpy array. En el caso particular que un numpy array contenga un unico valor, podemos castearlo a un float para convertirlo a un escalar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: conv_single_step\n",
    "\n",
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    Aplicar un filtro definido con parametros W a un solo slide (a_slice_prev) que sera la salida   \n",
    "    de la capa anterior.\n",
    "    \n",
    "    Parametros:\n",
    "    a_slice_prev -- slice de los datos de entrada con dimensiones (f, f, n_C_prev)\n",
    "    W -- Weight parametros de pesos de la ventana - matriz de dimensiones (f, f, n_C_prev)\n",
    "    b -- Bias parametros de bias de la ventana - matriz de dimensiones (1, 1, 1)\n",
    "    \n",
    "    Return:\n",
    "    Z -- un valor escalar, el resultado de la convolución de la ventana (W, b) sobre a_slice_prev de los datos de entrada\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    # Esta variable representa el producto entre a_slice_prev y W. No agregar Bias.\n",
    "    s = a_slice_prev * W\n",
    "    # Sum sobre todas las entradas del volumen s.\n",
    "    Z = np.sum(s, dtype=np.float)\n",
    "    # Sumo el Bias b a Z. Considerar la NOTA para que Z sea un escalar.\n",
    "    Z += float(b)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def truncate(number, digits) -> float:\n",
    "    stepper = 10.0 ** digits\n",
    "    return math.trunc(stepper * number) / stepper\n",
    "\n",
    "np.random.seed(1)\n",
    "a_slice_prev = np.random.randn(4, 4, 3)\n",
    "W = np.random.randn(4, 4, 3)\n",
    "b = np.random.randn(1, 1, 1)\n",
    "\n",
    "Z_aux = -6.99908945\n",
    "Z = conv_single_step(a_slice_prev, W, b)\n",
    "\n",
    "print(\"Z =\", truncate(Z, 8))\n",
    "\n",
    "assert(truncate(Z, 8) == Z_aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output**:\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            **Z**\n",
    "        </td>\n",
    "        <td>\n",
    "            -6.99908945068\n",
    "        </td>\n",
    "    </tr>\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Convolutional Neural Networks - Forward pass\n",
    "\n",
    "En el forward pass, tomaremos muchos filtros y realizaremos la convolución con el volumen de entrada. Cada 'convolución' nos dara una matriz de 2D de salida. Luego juntaremos estas salidas para obtener un volumen de 3D:\n",
    "\n",
    "<center>\n",
    "<video width=\"620\" height=\"440\" src=\"images/conv_kiank.mp4\" type=\"video/mp4\" controls>\n",
    "</video>\n",
    "</center>\n",
    "\n",
    "**Ejercicio**: \n",
    "Implementar la funcion que realizara la convolución entre los filtros `W` sobre la capa de activación previa `A_prev`.\n",
    "Esta función toma las siguientes entradas:\n",
    "* `A_prev`, la salida de la capa anterior (considerado como un conjunto de m); \n",
    "* Weights lo denotamos como `W`.  El filtro con tamaño de ventana `f` por `f`.\n",
    "* El vector de bias es `b`, donde cada filtro tiene su propio (solo un) bias. \n",
    "\n",
    "Además se agrega un diccionario de hyperparameters que contiene el valor del stride y del padding. \n",
    "\n",
    "**Pista**: \n",
    "1. Para seleccionar una slide de `2x2` en la esquina superior izquierda de la matriz \"a_prev\" (de dimension (5,5,3)), debemos hacer lo siguiente:\n",
    "```python\n",
    "a_slice_prev = a_prev[0:2,0:2,:]\n",
    "```\n",
    "Observar que esto genera un corte 3D que tiene altura 2, ancho 2 y profundidad 3. La profundidad es el número de canales.\n",
    "Esto es util para definir `a_slice_prev`, usar los indices `start/end` que definiremos.\n",
    "2. Para definir una slice necesitamos definir los vertices `vert_start`, `vert_end`, `horiz_start` y `horiz_end`. Esta figura puede sernos de ayuda para encontrar los vertices los cuales podemos definir usando h, w, f y s en el codigo.\n",
    "\n",
    "<img src=\"images/vert_horiz_kiank.png\" style=\"width:400px;height:300px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figura 3** </u><font color='purple'>  : **Definición de una slide usando vertical and horizontal start/end (con un filtro de 2x2)** <br> Esta figura muestra un solo canal.  </center></caption>\n",
    "\n",
    "\n",
    "**Formulas**:\n",
    "Estas formulas sobre las dimensiones del resultado de una convolución a partir de las dimensiones de la entrada:\n",
    "$$ n_H = \\lfloor \\frac{n_{H_{prev}} - f + 2 \\times pad}{stride} \\rfloor +1 $$\n",
    "$$ n_W = \\lfloor \\frac{n_{W_{prev}} - f + 2 \\times pad}{stride} \\rfloor +1 $$\n",
    "$$ n_C = \\text{number of filters used in the convolution}$$\n",
    "\n",
    "Para este ejericio, no nos preocuparemos por la implementación vectorizada, y vamos a implementar todo con for-loops ya que la idea base es lograr entender el funcionamiento de una CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consejos adicionales\n",
    "\n",
    "\n",
    "* Deberian utilizar el corte de matrices (ej.`varname[0:1,:,3:5]`) para las siguientes variables:  \n",
    "  `a_prev_pad` ,`W`, `b`  \n",
    "  Copiar el código de la función y ejecutarlo fuera de la función definida, en celdas separadas. Para comparar que el subconjunto de cada matriz es del tamaño y la dimensión que espera.\n",
    "* Para decidir cómo obtener vert_start, vert_end; horiz_start, horiz_end, recordar que estos son índices de la capa anterior.  \n",
    "  Los índices de la capa de salida se denotan con `h` y` w`.  \n",
    "* Asegúrarse de que `a_slice_prev` tenga altura, ancho y profundidad.\n",
    "* Recordar que `a_prev_pad` es un subconjunto de `A_prev_pad`.  \n",
    "  Pensar cuál debería usarse dentro de los bucles for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: conv_forward\n",
    "\n",
    "def relu(X):\n",
    "    return np.maximum(0,X)\n",
    "\n",
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    Implementar el forward propagation para una capa de convolución\n",
    "    \n",
    "    Parametros:\n",
    "    A_prev -- salida de activación de la capa anterior, \n",
    "        numpy array de dimensión (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W -- Weights, numpy array de dimensión (f, f, n_C_prev, n_C)\n",
    "    b -- Biases, numpy array de dimensión (1, 1, 1, n_C)\n",
    "    hparameters -- diccionario de python que contiene los valores de \"stride\" y \"pad\"\n",
    "        \n",
    "    Returns:\n",
    "    Z -- salida de la capa de convolución, numpy array de dimensiones (m, n_H, n_W, n_C)\n",
    "    cache -- cache de los valores necesarios para la función de conv_backward()\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Obtener dimensiones de A_prev's (≈1 linea)  \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    # Obtener dimensiones de W's (≈1 linea)\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Obtener informacion de los \"hparameters\" (≈2 lineas)\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    # Calcular las dimensiones de la salida de la capa de CONV usando las formulas vistas arriba. \n",
    "    # Pista: usar int() para aplicar el 'floor' de la función. (≈2 lineas)\n",
    "    n_H = int((n_H_prev - f + 2*pad) / stride) + 1\n",
    "    n_W = int((n_W_prev - f + 2*pad) / stride) + 1\n",
    "    \n",
    "    # Inicializar el volumen de salida Z con zeros. (≈1 linea)\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Crear A_prev_pad con el padding aplicado a A_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    \n",
    "    for i in range(0, m):                   # loop sobre el conjunto de ejemplos de entrenamiento\n",
    "        a_prev_pad = A_prev_pad[i] # Seleccionar el i-esimo ejemplo de entrenamiento con el padding aplicado\n",
    "        for h in range(0, n_H):             # loop sobre el eje vertical del volumen de salida\n",
    "            # Inicializar la arista vertical del correspondiente \"slice\" (≈2 lineas)\n",
    "            vert_start = h * stride\n",
    "            vert_end = vert_start + f\n",
    "            \n",
    "            for w in range(0, n_W):       # loop sobre el eje horizontal del volumen de salida\n",
    "                # Inicializar la arista horizontal del correspondiente \"slice\" (≈2 lineas)\n",
    "                horiz_start = w * stride\n",
    "                horiz_end = horiz_start + f\n",
    "                \n",
    "                for c in range(0, n_C):   # loop sobre los canales (= #filters) del volumen de salida\n",
    "                                        \n",
    "                    # Usar la esquina para definir el slice (3D) de a_prev_pad. (≈1 linea)\n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "                    \n",
    "                    # Convolucionar la slice (3D) con el filtro correcto W y bias b, para obtener una neurona de salida. (≈3 lineas)\n",
    "                    weights = W[:,:,:,c]\n",
    "                    biases = b[:,:,:,c]\n",
    "                    Z[i, h, w, c] = relu(conv_single_step(a_slice_prev, weights, biases))\n",
    "                                        \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Guardamos la información en \"cache\" para el backprop\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, la capa CONV también debe contener una activación, en cuyo caso agregaríamos la siguiente línea de código:\n",
    "\n",
    "```python\n",
    "# Aplicar la función de activación\n",
    "A[i, h, w, c] = activation(Z[i, h, w, c])\n",
    "```\n",
    "\n",
    "Esto no es necesario en este ejercicio.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Pooling layer \n",
    "\n",
    "La capa de agrupación (POOL) reduce la altura y el ancho de la entrada. Ayuda a reducir el computo necesario, así como a hacer que los detectores de características sean más invariantes a la posición en la entrada. Los dos tipos de capas de agrupación son:\n",
    "\n",
    "- Max-pooling layer: en las slides de entrada aplicamos ventanas de dimensiones ($f, f$) y almacenamos el maximo valor de la ventana en la salida.\n",
    "\n",
    "- Average-pooling layer: en las slides de entrada aplicamos ventanas de dimensiones ($f, f$) y almacenamos el valor promedio de la ventana en la salida.\n",
    "\n",
    "<table>\n",
    "<td>\n",
    "<img src=\"images/max_pool1.png\" style=\"width:500px;height:300px;\">\n",
    "<td>\n",
    "\n",
    "<td>\n",
    "<img src=\"images/a_pool.png\" style=\"width:500px;height:300px;\">\n",
    "<td>\n",
    "</table>\n",
    "\n",
    "Estas capas de agrupación no tienen parámetros para que la retropropagación entrene. Sin embargo, tienen hiperparámetros como el tamaño de la ventana $f$. Esto especifica la altura y el ancho de la ventana $f \\times f$ en la que calcularía el *maximo* o *promedio*.\n",
    "\n",
    "### 4.1 - Forward Pooling\n",
    "Ahora, vamos a implementar MAX-POOL y AVG-POOL, en la misma función. \n",
    "\n",
    "**Ejercicio**: Implementar la forward pass sobre una pooling layer.\n",
    "\n",
    "**Recordatorio**:\n",
    "Las formulas sobre la dimensión del volumen de salida en relación al volumen de entrada son:\n",
    "\n",
    "$$ n_H = \\lfloor \\frac{n_{H_{prev}} - f}{stride} \\rfloor +1 $$\n",
    "\n",
    "$$ n_W = \\lfloor \\frac{n_{W_{prev}} - f}{stride} \\rfloor +1 $$\n",
    "\n",
    "$$ n_C = n_{C_{prev}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUNCTION: pool_forward\n",
    "\n",
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implementar la forward pass de la pooling layer\n",
    "    \n",
    "    Parametros:\n",
    "    A_prev -- Datos de entrada, numpy array de dimension (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    hparameters -- diccionario de python contiene los valores de \"f\" y \"stride\"\n",
    "    mode -- el pooling mode que te gustaria usar, definido como un string que puede tomar estos valores (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    A -- salida de la pool layer, numpy array de dimensión (m, n_H, n_W, n_C)\n",
    "    cache -- cache usado en el backward pass de la pooling layer, contiene la entrada y los hparameters \n",
    "    \"\"\"\n",
    "    \n",
    "    # Obtener las dimensiones del volumen de entrada\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Obtener los hyperparameters de \"hparameters\"\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "    # Definir las dimensiones de la salida\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    # Inicializar la matriz de salida A\n",
    "    A = np.zeros((m, n_H, n_W, n_C))              \n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    for i in range(0, m):                         # loop sobre los ejemplos de entrenamiento\n",
    "        a_prev = A_prev[i]\n",
    "        for h in range(0, n_H):                     # loop sobre el eje vertical del volumen de salida\n",
    "            # Calcular la arista vertical de la \"slice\" (≈2 lineas)\n",
    "            vert_start = h * stride\n",
    "            vert_end = vert_start + f\n",
    "            \n",
    "            for w in range(0, n_W):                 # loop sobre el eje horizontal del volumen de salida\n",
    "                # Calcular la arista horizontal de la \"slice\" (≈2 lines)\n",
    "                horiz_start = w * stride\n",
    "                horiz_end = horiz_start + f\n",
    "                \n",
    "                for c in range (n_C):            # loop sobre los canales del volumen de salida\n",
    "                    \n",
    "                    # Usar la esquina definida por el slice en el i-esimo ejemplo de entrenamiento A_prev, del canal c. (≈1 linea)\n",
    "                    a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    \n",
    "                    # Calcular el pooling sobre el slice. \n",
    "                    # Usamos un if para diferenciar el modo. \n",
    "                    # Calcular el maximo y la media del NUMPY ARRAY.\n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Guardamos la entrada y los hparameters en el \"cache\" para el pool_backward()\n",
    "    cache = (A_prev, hparameters)\n",
    "    \n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have now implemented the forward passes of all the layers of a convolutional network. \n",
    "\n",
    "The remainder of this notebook is optional, and will not be graded.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Backpropagation in convolutional neural networks (OPTIONAL / UNGRADED)\n",
    "\n",
    "In modern deep learning frameworks, you only have to implement the forward pass, and the framework takes care of the backward pass, so most deep learning engineers don't need to bother with the details of the backward pass. The backward pass for convolutional networks is complicated. If you wish, you can work through this optional portion of the notebook to get a sense of what backprop in a convolutional network looks like. \n",
    "\n",
    "When in an earlier course you implemented a simple (fully connected) neural network, you used backpropagation to compute the derivatives with respect to the cost to update the parameters. Similarly, in convolutional neural networks you can calculate the derivatives with respect to the cost in order to update the parameters. The backprop equations are not trivial and we did not derive them in lecture, but we will briefly present them below.\n",
    "\n",
    "### 5.1 - Convolutional layer backward pass \n",
    "\n",
    "Let's start by implementing the backward pass for a CONV layer. \n",
    "\n",
    "#### 5.1.1 - Computing dA:\n",
    "This is the formula for computing $dA$ with respect to the cost for a certain filter $W_c$ and a given training example:\n",
    "\n",
    "$$ dA += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^{n_W} W_c \\times dZ_{hw} \\tag{1}$$\n",
    "\n",
    "Where $W_c$ is a filter and $dZ_{hw}$ is a scalar corresponding to the gradient of the cost with respect to the output of the conv layer Z at the hth row and wth column (corresponding to the dot product taken at the ith stride left and jth stride down). Note that at each time, we multiply the the same filter $W_c$ by a different dZ when updating dA. We do so mainly because when computing the forward propagation, each filter is dotted and summed by a different a_slice. Therefore when computing the backprop for dA, we are just adding the gradients of all the a_slices. \n",
    "\n",
    "In code, inside the appropriate for-loops, this formula translates into:\n",
    "```python\n",
    "da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "```\n",
    "\n",
    "#### 5.1.2 - Computing dW:\n",
    "This is the formula for computing $dW_c$ ($dW_c$ is the derivative of one filter) with respect to the loss:\n",
    "\n",
    "$$ dW_c  += \\sum _{h=0} ^{n_H} \\sum_{w=0} ^ {n_W} a_{slice} \\times dZ_{hw}  \\tag{2}$$\n",
    "\n",
    "Where $a_{slice}$ corresponds to the slice which was used to generate the activation $Z_{ij}$. Hence, this ends up giving us the gradient for $W$ with respect to that slice. Since it is the same $W$, we will just add up all such gradients to get $dW$. \n",
    "\n",
    "In code, inside the appropriate for-loops, this formula translates into:\n",
    "```python\n",
    "dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "```\n",
    "\n",
    "#### 5.1.3 - Computing db:\n",
    "\n",
    "This is the formula for computing $db$ with respect to the cost for a certain filter $W_c$:\n",
    "\n",
    "$$ db = \\sum_h \\sum_w dZ_{hw} \\tag{3}$$\n",
    "\n",
    "As you have previously seen in basic neural networks, db is computed by summing $dZ$. In this case, you are just summing over all the gradients of the conv output (Z) with respect to the cost. \n",
    "\n",
    "In code, inside the appropriate for-loops, this formula translates into:\n",
    "```python\n",
    "db[:,:,:,c] += dZ[i, h, w, c]\n",
    "```\n",
    "\n",
    "**Exercise**: Implement the `conv_backward` function below. You should sum over all the training examples, filters, heights, and widths. You should then compute the derivatives using formulas 1, 2 and 3 above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "          numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "          numpy array of shape (1, 1, 1, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve information from \"cache\"\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\"\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    # Retrieve dimensions from dZ's shape\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    # Initialize dA_prev, dW, db with the correct shapes\n",
    "    dA_prev = np.zeros(A_prev.shape)                           \n",
    "    dW = np.zeros(W.shape)   \n",
    "    db = np.zeros(b.shape)   \n",
    "\n",
    "    # Pad A_prev and dA_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select ith training example from A_prev_pad and dA_prev_pad\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "        \n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):           # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the slice from a_prev_pad\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                    # Update gradients for the window and the filter's parameters using the code formulas given above\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                    \n",
    "        # Set the ith training example's dA_prev to the unpadded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])\n",
    "        dA_prev[i, :, :, :] = dA_prev_pad[i, pad:-pad, pad:-pad, :]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Pooling layer - backward pass\n",
    "\n",
    "Next, let's implement the backward pass for the pooling layer, starting with the MAX-POOL layer. Even though a pooling layer has no parameters for backprop to update, you still need to backpropagation the gradient through the pooling layer in order to compute gradients for layers that came before the pooling layer. \n",
    "\n",
    "### 5.2.1 Max pooling - backward pass  \n",
    "\n",
    "Before jumping into the backpropagation of the pooling layer, you are going to build a helper function called `create_mask_from_window()` which does the following: \n",
    "\n",
    "$$ X = \\begin{bmatrix}\n",
    "1 && 3 \\\\\n",
    "4 && 2\n",
    "\\end{bmatrix} \\quad \\rightarrow  \\quad M =\\begin{bmatrix}\n",
    "0 && 0 \\\\\n",
    "1 && 0\n",
    "\\end{bmatrix}\\tag{4}$$\n",
    "\n",
    "As you can see, this function creates a \"mask\" matrix which keeps track of where the maximum of the matrix is. True (1) indicates the position of the maximum in X, the other entries are False (0). You'll see later that the backward pass for average pooling will be similar to this but using a different mask.  \n",
    "\n",
    "**Exercise**: Implement `create_mask_from_window()`. This function will be helpful for pooling backward. \n",
    "Hints:\n",
    "- [np.max()]() may be helpful. It computes the maximum of an array.\n",
    "- If you have a matrix X and a scalar x: `A = (X == x)` will return a matrix A of the same size as X such that:\n",
    "```\n",
    "A[i,j] = True if X[i,j] = x\n",
    "A[i,j] = False if X[i,j] != x\n",
    "```\n",
    "- Here, you don't need to consider cases where there are several maxima in a matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "    \"\"\"\n",
    "    Creates a mask from an input matrix x, to identify the max entry of x.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- Array of shape (f, f)\n",
    "    \n",
    "    Returns:\n",
    "    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈1 line)\n",
    "    mask = (x == np.max(x))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we keep track of the position of the max? It's because this is the input value that ultimately influenced the output, and therefore the cost. Backprop is computing gradients with respect to the cost, so anything that influences the ultimate cost should have a non-zero gradient. So, backprop will \"propagate\" the gradient back to this particular input value that had influenced the cost. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.2 - Average pooling - backward pass \n",
    "\n",
    "In max pooling, for each input window, all the \"influence\" on the output came from a single input value--the max. In average pooling, every element of the input window has equal influence on the output. So to implement backprop, you will now implement a helper function that reflects this.\n",
    "\n",
    "For example if we did average pooling in the forward pass using a 2x2 filter, then the mask you'll use for the backward pass will look like: \n",
    "$$ dZ = 1 \\quad \\rightarrow  \\quad dZ =\\begin{bmatrix}\n",
    "1/4 && 1/4 \\\\\n",
    "1/4 && 1/4\n",
    "\\end{bmatrix}\\tag{5}$$\n",
    "\n",
    "This implies that each position in the $dZ$ matrix contributes equally to output because in the forward pass, we took an average. \n",
    "\n",
    "**Exercise**: Implement the function below to equally distribute a value dz through a matrix of dimension shape. [Hint](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.ones.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_value(dz, shape):\n",
    "    \"\"\"\n",
    "    Distributes the input value in the matrix of dimension shape\n",
    "    \n",
    "    Arguments:\n",
    "    dz -- input scalar\n",
    "    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n",
    "    \n",
    "    Returns:\n",
    "    a -- Array of size (n_H, n_W) for which we distributed the value of dz\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from shape (≈1 line)\n",
    "    (n_H, n_W) = shape\n",
    "    \n",
    "    # Compute the value to distribute on the matrix (≈1 line)\n",
    "    average = dz/(n_H + n_W)\n",
    "    \n",
    "    # Create a matrix where every entry is the \"average\" value (≈1 line)\n",
    "    a = np.ones(shape) * average\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2.3 Putting it together: Pooling backward \n",
    "\n",
    "You now have everything you need to compute backward propagation on a pooling layer.\n",
    "\n",
    "**Exercise**: Implement the `pool_backward` function in both modes (`\"max\"` and `\"average\"`). You will once again use 4 for-loops (iterating over training examples, height, width, and channels). You should use an `if/elif` statement to see if the mode is equal to `'max'` or `'average'`. If it is equal to 'average' you should use the `distribute_value()` function you implemented above to create a matrix of the same shape as `a_slice`. Otherwise, the mode is equal to '`max`', and you will create a mask with `create_mask_from_window()` and multiply it by the corresponding value of dA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the backward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
    "    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters \n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Retrieve information from cache (≈1 line)\n",
    "    (A_prev, hparameters) = cache\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\" (≈2 lines)\n",
    "    stride = hparameters[\"stride\"]\n",
    "    f = hparameters[\"f\"]\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape and dA's shape (≈2 lines)\n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    # Initialize dA_prev with zeros (≈1 line)\n",
    "    dA_prev = np.zeros(A_prev.shape)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select training example from A_prev (≈1 line)\n",
    "        a_prev = A_prev[i]\n",
    "        \n",
    "        for h in range(n_H):                   # loop on the vertical axis\n",
    "            for w in range(n_W):               # loop on the horizontal axis\n",
    "                for c in range(n_C):           # loop over the channels (depth)\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Compute the backward propagation in both modes.\n",
    "                    if mode == \"max\":\n",
    "                        \n",
    "                        # Use the corners and \"c\" to define the current slice from a_prev (≈1 line)\n",
    "                        a_prev_slice = a_prev[vert_start: vert_end, horiz_start: horiz_end, c]\n",
    "                        # Create the mask from a_prev_slice (≈1 line)\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "                        # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) (≈1 line)\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += mask * dA[i, h, w, c]\n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        \n",
    "                        # Get the value a from dA (≈1 line)\n",
    "                        da = dA[i, h, w, c]\n",
    "                        # Define the shape of the filter as fxf (≈1 line)\n",
    "                        shape = (f,f)\n",
    "                        # Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da. (≈1 line)\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(da, shape)\n",
    "                        \n",
    "    ### END CODE ###\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y = load_data_real()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 15000\n",
    "plt.imshow(train_x_orig[index])\n",
    "if train_y[0,index] == 1:\n",
    "    print (\"y = \" + str(train_y[0,index]) + \". Es la imagen de un gato.\")\n",
    "else:\n",
    "    print (\"y = \" + str(train_y[0,index]) + \". NO es la imagen de un gato.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_train = train_x_orig.shape[0]\n",
    "num_px = train_x_orig.shape[1]\n",
    "m_test = test_x_orig.shape[0]\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples: \" + str(m_test))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
    "print (\"train_y shape: \" + str(train_y.shape))\n",
    "print (\"test_x_orig shape: \" + str(test_x_orig.shape))\n",
    "print (\"test_y shape: \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fit(X, y, epochs = 4, learning_rate=0.0075):\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    a_slice_prev = np.random.randn(4, 4, 3)\n",
    "    W1 = np.random.randn(7, 7, 3, 10)\n",
    "    b1 = np.random.randn(1, 1, 1, 10)\n",
    "    \n",
    "    W2 = np.random.randn(7, 7, 10, 15)\n",
    "    b2 = np.random.randn(1, 1, 1, 15)\n",
    "    \n",
    "    hparameters1 = {\n",
    "        \"f\": 4,\n",
    "        \"stride\": 1,\n",
    "        \"pad\": 1\n",
    "    }\n",
    "    hparameters2 = {\n",
    "        \"f\":3,\n",
    "        \"stride\": 2,\n",
    "        \"pad\": 2\n",
    "    }\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        print(f\"Epoch {i} start\")\n",
    "        \n",
    "        A1, cache1 = conv_forward(X, W1, b1, hparameters1)\n",
    "        A_pool_1, cache_pool_1 = pool_forward(A1, hparameters1, mode = \"max\")\n",
    "        print(f\"Epoch {i} 20%\")\n",
    "        A2, cache2 = con_forward(A_pool_1, W2, b2, hparameters2)\n",
    "        A_pool_2, cache_pool_2 = pool_forward(A2, hparameters2, mode = \"average\")\n",
    "        print(f\"Epoch {i} 40%\")\n",
    "        dA_2 = pool_backward(A_pool_2, cache_pool_2, mode = \"average\")\n",
    "        dA_prev_2, dW2, db2 = conv_backward(dA_2, cache2)\n",
    "        print(f\"Epoch {i} 60%\")\n",
    "        \n",
    "        dA_1 = pool_backward(dA_prev_2, cache_pool_1, mode = \"max\")\n",
    "        dA_prev1, dW1, db1 = conv_backward(dA_1, cache1)\n",
    "        print(f\"Epoch {i} 85%\")\n",
    "        W1 -= learning_rate * dW1\n",
    "        b1 -= learning_rate * db1\n",
    "        \n",
    "        W2 -= learning_rate * dW2\n",
    "        b2 -= learning_rate * db2\n",
    "        print(f\"Epoch {i} 100%\")\n",
    "        \n",
    "    # Redimensionamos los conjuntos\n",
    "    train_x_flatten = A_pool_2.reshape(A_pool_2.shape[0], -1, -1).T   # El \"-1\" indica que se desea aplanar la curva! \n",
    "    print(train_x_flatten.shape)\n",
    "    # Estandarizamos los valores de las features entre 0 y 1.\n",
    "    train_x = train_x_flatten\n",
    "    \n",
    "    print(\"train_x's shape: \" + str(train_x.shape))\n",
    "    print(\"test_x's shape: \" + str(test_x.shape))\n",
    "    \n",
    "    layers_dims = [20, 9] #  4-layer model\n",
    "    deep_neural_network = DeepNeuralNetwork()\n",
    "    deep_neural_network.fit(train_x, y, learning_rate = 0.0075, num_iterations = 2500, layer_hidden_neurons = layers_dims, activation=\"relu\")\n",
    "    \n",
    "    return deep_neural_network\n",
    "        \n",
    "        \n",
    "deep_neural_network = fit(train_x_orig, train_y)"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "convolutional-neural-networks",
   "graded_item_id": "qO8ng",
   "launcher_item_id": "7XDi8"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}